- [首页](../README.md "Java Interview Docs")
1. 大模型推理过程
    - Tokenization（分词）：通过字节对编码（BPE）分词方法将文本文本分解成token，每个词元都对应一个模型可以使用的整数 ID：token ID
    - Embedding向量化：每个 token ID 到 embedding_matrix 查找对应向量，把单词转换成它在高维空间中的语义位置
    - 加入Positional Encoding：让模型理解句子结构
    - Prefill 阶段：输入向量值会一次性通过所有层（例如 32 层）
    - 预测第一个输出 token：经过最后一层 Transformer 后，hidden_states 会投影到词表大小
    - Decode 阶段（逐 token 自回归生成）：生成一个 token 后，为了生成下一个 token只需为“新 token”计算一次 Q、K、V（注意力输入），之前 token 的 K 和 V 都从 KV Cache 中读取，不需要再算
    - Detokenization：把生成序列的 token ID解码成自然语言文本
2. 为什么不能直接以"字符"或"单词"作为模型的输入单元
3. BPE（Byte Pair Encoding）算法解决了什么问题
4. 自注意力机制（Self-Attention）的核心思想是什么
5. Positional Encoding主要解决了什么问题
6. 智能体范式
    - ReAct (Reasoning and Acting)： 一种将“思考”和“行动”紧密结合的范式，让智能体边想边做，动态调整（走一步，看一步）
    - Plan-and-Solve： 一种“三思而后行”的范式，智能体首先生成一个完整的行动计划，然后严格执行（一次性生成完整计划）
    - Reflection： 一种赋予智能体“反思”能力的范式，通过自我批判和修正来优化结果（以成本换质量）
7. ReAct和传统的思维链 (Chain-of-Thought)有什么区别
    - CoT无法与外部世界交互
    - ReAct不断重复 Thought -> Action -> Observation 的循环，将新的观察结果追加到历史记录中，形成一个不断增长的上下文，直到它在 Thought 中认为已经找到了最终答案，然后输出结果
8. Reflection 机制的核心思想
   - 执行 (Execution)：首先，智能体使用我们熟悉的方法（如 ReAct 或 Plan-and-Solve）尝试完成任务，生成一 个初步的解决方案或行动轨迹。这可以看作是“初稿”。
   - 反思 (Reflection)：接着，智能体进入反思阶段。它会调用一个独立的、或者带有特殊提示词的大语言模型实 例，来扮演一个“评审员”的角色。
     - 这个“评审员”会审视第一步生成的“初稿”，并从多个维度进行评估，例如： 
       - 事实性错误：是否存在与常识或已知事实相悖的内容？
       - 逻辑漏洞：推理过程是否存在不连贯或矛盾之处？
       - 效率问题：是否有更直接、更简洁的路径来完成任务？
       - 遗漏信息：是否忽略了问题的某些关键约束或方面？ 
     - 根据评估，它会生成一段结构化的反馈(Feedback)，指出具体的问题所在和改进建议。
   - 优化 (Refinement)：最后，智能体将“初稿”和“反馈”作为新的上下文，再次调用大语言模型，要求它根据反馈内容对初稿进行修正，生成一个更完善的“修订稿”
   - 这个循环可以重复进行多次，直到反思阶段不再发现新的问题，或者达到预设的迭代次数上限
9. 在ReAct范式中，当可调用工具数量随业务需求显著增加时，从工程角度如何优化工具的组织和检索机制
10. Plan-and-Solve如果在执行过程中发现某个步骤无法完成或结果不符合预期，应该如何设计一个"动态重规划"机制？
11. Reflection 机制的终止条件是"反馈中包含无需改进"或"达到最大迭代次数"。这种设计是否合理？能否设计一个更智能的终止条件？
12. 智能体主流框架
    - AutoGen
      - AutoGen 的核心思想是通过对话实现协作。它将多智能体系统抽象为一个由多个“可对话”智能体组成的群聊。
      - 开发者可以定义不同角色（如 Coder , ProductManager , Tester ），并设定它们之间的交互规则（例如， Coder 写完代码后由 Tester 自动接管）。
      - 任务的解决过程，就是这些智能体在群聊中通过自动化消息传递，不断对话、协作、迭代直至最终目标达成的过程
    - AgentScope 
      - AgentScope 的核心创新在于其消息驱动架构。在这个架构中，所有的智能体交互都被抽象为消息的发送和接收，而不是传统的函数调用。
      - AgentScope 内置了一个消息中心 (MsgHub)，它是整个消息驱动架构的中枢，负责智能体间的消息路由和状态管理
      - 异步解耦: 消息的发送方和接收方在时间上解耦，无需相互等待，天然支持高并发场景
    - CAMEL
      - 最初的核心目标是探索如何在最少的人类干预下，让两个智能体通过“角色扮演”自主协作解决复杂任务
      - 两大核心概念：角色扮演 (Role-Playing) 和 引导性提示 (Inception Prompting)
      - 在 CAMEL 最初的设计中，一个任务通常由两个智能体协作完成。这两个智能体被赋予了互补的、明确定义的“角色”。 
        - 一个扮演“AI 用户” (AI User)，负责提出需求、下达指令和构思任务步骤；
        - 另一个则扮演“AI 助理” (AI Assistant)，负责根据指令执行具体操作和提供解决方案
    - LangGraph
      - LangGraph 将智能体的执行流程建模为一种状态机（State Machine），并将其表示为有向图（Directed Graph）。
      - 在这种范式中，图的节点（Nodes）代表一个具体的计算步骤（如调用LLM、执行工具），而边（Edges）则定义了从一个节点到另一个节点的跳转逻辑。
      - 这种设计的革命性之处在于它天然支持循环
      - 三个基本构成要素：
        - 全局状态（State）。整个图的执行过程都围绕一个共享的状态对象进行。这个状态通常包含任何需要追踪的信息，如对话历史、中间结果、迭代次数等。所有的节点都能读取和更新这个中心状态
        - 节点（Nodes）。每个节点都是一个接收当前状态作为输入、并返回一个更新后的状态作为输出的函数。节点是执行具体工作的单元。
        - 边（Edges）。边负责连接节点，定义工作流的方向。最简单的边是常规边，它指定了一个节点的输出总是流向另一个固定的节点。而 LangGraph 最强大的功能在于条件边（Conditional Edges）。它通过一个函数来判断当前的状态，然后动态地决定下一步应该跳转到哪个节点。这正是实现循环和复杂逻辑分支的关键
13. 智能体框架核心模块： 
    - 智能体 = 在循环中自主调用工具的 LLM
    - 统一LLM接口、标准化消息系统、工具注册机制，共同构成了一个完备的技术底座
    - 记忆系统
      - 工作记忆 (Working Memory)，它扮演着智能体“短期记忆”的角色，主要用于存储当前对话的上下文信息。为确保高速访问和响应，其容量被有意限制（例如，默认50条），并且生命周期与单个会话绑定，会话结束后便会自动清理。
      - 情景记忆 (Episodic Memory)，它负责长期存储具体的交互事件和智能体的学习经历。与工作记忆不同，情景记忆包含了丰富的上下文信息，并支持按时间序列或主题进行回顾式检索，是智能体“复盘”和学习过往经验的基础。
      - 语义记忆 (Semantic Memory)，它存储的是更为抽象的知识、概念和规则。例如，通过对话了解到的用户偏好、需要长期遵守的指令或领域知识点，都适合存放在这里
    - RAG检索
    - 上下文工程
    - 智能体协议（MCP、A2A）
14. RAG检索策略
    - 多查询扩展（MQE）：通过生成语义等价的多样化查询来提高检索召回率，并行执行这些扩展查询并合并结果，系统能够覆盖更广泛的相关文档，避免因用词差异而遗漏重要信息
    - 假设文档嵌入（HyDE）：它的核心思想是"用答案找答案"。HyDE通过让LLM先生成一个假设性的答案段落，然后用这个答案段落去检索真实文档，从而缩小了查询和文档之间的语义鸿沟。这种方法的优势在于，假设答案与真实答案在语义空间中更加接近，因此能够更准确地匹配到相关文档
15. 上下文工程 vs 提示工程
    - 提示工程关注如何编写与组织 LLM 的指令以获得更优结果（例如系统提示的写法与结构化策略），提示工程的核心是“如何写出有效提示”，尤其是系统提示
    - 上下文工程是提示工程的自然演进，需要管理整个上下文状态的策略——其中包括系统指令、工具、MCP（Model Context Protocol）、外部数据、消息历史等
    - 一个循环运行的智能体，会不断产生下一轮推理可能相关的数据，这些信息必须被周期性地提炼。因此，上下文工程的核心在于从持续扩张的“候选信息宇宙”中，甄别哪些内容应当进入有限的上下文窗口。
16. 上下文腐蚀（context rot）——随着上下文窗口中的 tokens 增加，模型从上下文中准确回忆信息的能力反而下降。
    - 上下文必须被视作一种有限资源，且具有边际收益递减
    - 源自 LLM 的架构约束。Transformer 让每个 token 能够与上下文中的所有 token 建立关联，理论上形成n平方级别的两两注意力关系。
    - 随着上下文长度增长，模型对这些两两关系的建模能力会被“拉薄”，从而自然地产生“上下文规模”与“注意力集中度”的张力。
    - 此外，模型的注意力模式来源于训练数据分布——短序列通常比长序列更常见，因此模型对“全上下文依赖”的经验更少、专门参数也更少
    - 这些因素共同形成的是一个性能梯度，而非“悬崖式”崩溃：模型在长上下文下依旧强大，但相较短上下文，在信息检索与长程推理上的精度会有所下降
17. 上下文工程的策略
    - 混合策略：前置加载少量“高价值”上下文以保证速度，然后允许智能体按需继续自主探索。边界的选择取决于任务动态性与时效要求。
      - 在工程上，可以预先放入类似“项目约定说明（如 README/指南）”的文件，同时提供 glob 、 grep 等原语，让智能体即时检索具体文件，从而绕开过时索引与复杂语法树的沉没成本
    - 压缩整合：让模型压缩并保留架构性决策、未解决缺陷、实现细节，丢弃重复的工具输出与噪声；新窗口携带压缩摘要 + 最近少量高相关工件
    - 记忆系统：智能体以固定频率将关键信息写入上下文外的持久化存储，在后续阶段按需拉回。
    - 子代理架构：由主代理负责高层规划与综合，多个专长子代理在“干净的上下文窗口”中各自深挖、调用工具并探索，最后仅回传凝练摘要（常见 1,000–2,000 tokens）
18. 上下文候选构造
    - 添加系统指令
    - 从记忆系统检索相关记忆
    - 从 RAG 系统检索相关知识
    - 添加对话历史(仅保留最近的 N 条)
    - 添加自定义信息包







